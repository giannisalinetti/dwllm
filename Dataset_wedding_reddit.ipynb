{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eb2FMDBOp7Yk",
        "Fz0YtQzjqCZM"
      ],
      "authorship_tag": "ABX9TyPBSyCdqP7tDljmD97kN2C7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giannisalinetti/dwllm/blob/main/Dataset_wedding_reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subreddit Dataset generator for BidBudgetBrides"
      ],
      "metadata": {
        "id": "GDx0Bp4Tpr5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape data from Reddit"
      ],
      "metadata": {
        "id": "eb2FMDBOp7Yk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6DbUoU_KQm_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QETnvTVLKpic"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import time\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    check_for_async = False,\n",
        "    client_id = userdata.get('reddit_client_id'),\n",
        "    client_secret = userdata.get('reddit_client_secret'),\n",
        "    user_agent = \"dataset_scraper/1.0\",\n",
        ")\n",
        "\n",
        "subreddit_name = \"BigBudgetBrides\"\n",
        "\n",
        "def fetch_hot_posts(subreddit_name, retries=3):\n",
        "    \"\"\"\n",
        "    Fetches hot posts from a given subreddit and their comments, with error handling.\n",
        "\n",
        "    Args:\n",
        "        subreddit_name (str): The name of the subreddit.\n",
        "        retries (int): The number of retries for API requests.\n",
        "    \"\"\"\n",
        "\n",
        "    posts = []\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            for post in reddit.subreddit(subreddit_name).hot(limit=None):\n",
        "                post.comments.replace_more(limit=None)\n",
        "\n",
        "                for comment in post.comments:\n",
        "                    posts.append([post.title,\n",
        "                                  post.score,\n",
        "                                  post.url,\n",
        "                                  post.num_comments,\n",
        "                                  post.selftext,\n",
        "                                  comment.body,\n",
        "                                  comment.score,\n",
        "                                  comment.created_utc\n",
        "                    ])\n",
        "                time.sleep(2)\n",
        "            break\n",
        "        except praw.exceptions.APIException as e:\n",
        "            if \"429\" in str(e):\n",
        "                wait_time = 60  # Wait 1 minute before retrying\n",
        "                print(f\"Rate limit hit! Waiting {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"Unexpected error: {e}\")\n",
        "                break  # Stop if another error occurs\n",
        "    return posts\n",
        "\n",
        "retrieved_posts = fetch_hot_posts(subreddit_name)\n"
      ],
      "metadata": {
        "id": "oUzUo89PKz76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the dataframe and cleanup"
      ],
      "metadata": {
        "id": "Fz0YtQzjqCZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe\n",
        "df = pd.DataFrame(retrieved_posts, columns=[\"title\",\n",
        "                                  \"score\",\n",
        "                                  \"url\",\n",
        "                                  \"num_comments\",\n",
        "                                  \"selftext\",\n",
        "                                  \"comment_body\",\n",
        "                                  \"comment_score\",\n",
        "                                  \"comment_timestamp\"])\n",
        "\n",
        "\n",
        "# Drop duplicate comments\n",
        "df.drop_duplicates(subset=[\"comment_body\"], inplace=True)\n",
        "\n",
        "# Drop missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Reset index\n",
        "df.reset_index(drop=True, inplace=True)\n",
        ""
      ],
      "metadata": {
        "id": "bNHeNPmNMowq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize text data\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_and_lemmatize_text(text):\n",
        "  text = str(text).lower()\n",
        "  text = re.sub(r\"http\\S+|www.\\S+\", \"\", text) # Remove URLs\n",
        "  text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Remove special characters\n",
        "\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "  # Remove stopwords and lemmatize words\n",
        "  cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
        "  ]\n",
        "\n",
        "  # Reconstruct sentence\n",
        "  return \" \".join(cleaned_tokens)\n",
        "\n",
        "\n",
        "# Apply cleaning function\n",
        "df[\"comment_body_clean\"] = df[\"comment_body\"].apply(clean_and_lemmatize_text)\n",
        "\n",
        "# Convert timestamps to readable format\n",
        "from datetime import datetime\n",
        "df[\"comment_timestamp\"] = pd.to_datetime(df[\"comment_timestamp\"], unit=\"s\")\n"
      ],
      "metadata": {
        "id": "8zE_XapGg0tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save local CSV"
      ],
      "metadata": {
        "id": "xj87iJO3qLPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path = \"/content/drive/My Drive/datasets/big_budget_brides.csv\"\n",
        "\n",
        "df.to_csv(save_path, index=False)"
      ],
      "metadata": {
        "id": "jk3BvGBKOBQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to HuggingFace dataset format"
      ],
      "metadata": {
        "id": "96VdP705oxLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "ALxgYfmKojm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/datasets/big_budget_brides.csv\")\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Save as Hugging Face dataset format\n",
        "dataset.save_to_disk(\"/content/drive/My Drive/datasets/big_budget_brides_hf\")\n",
        "print(\"Dataset saved successfully!\")\n"
      ],
      "metadata": {
        "id": "_bzTTS-So4tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to HuggingFace"
      ],
      "metadata": {
        "id": "DhUT7YsVul7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "KfQgu328uovf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "from google.colab import userdata\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Save the token to use it for authentication\n",
        "HfFolder.save_token(HF_TOKEN)\n",
        "\n",
        "# Initialize API with token\n",
        "api = HfApi(token=HF_TOKEN)\n"
      ],
      "metadata": {
        "id": "LHauIpYDut2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push dataset to Hugging Face\n",
        "repo_id = \"gbsalinetti/bigbudgetbrides-reddit-dataset\"\n",
        "dataset.push_to_hub(repo_id, token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "kYXwiB3VvxSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}